{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Barts Health Data Platform Documentation","text":""},{"location":"#overview","title":"Overview","text":"<p>Our documentation is spread across the three main functions within the Barts Health Data Platform (BHDP). These are:</p> <ul> <li>The Data Portal for making Data Access Requests and managing your project once approved.</li> <li>The Analytics Data Core which transforms the data that you will be authorised access to.</li> <li>Our Secure Data Environment based in Azure where you will be able to analyse the data we provide.</li> </ul> <p>If you have issues with any of these services or documentation then please contact XXX</p>"},{"location":"hi/","title":"Hi","text":"<p>hello again</p>"},{"location":"adc/Home/","title":"The Analysis Data Core","text":"<p>This page contains the user documentation around the Analysis Data Core (ADC).</p>"},{"location":"adc/Home/#introduction","title":"Introduction","text":"<p>The ADC refers to the data warehouse environment where we transfrom the raw patient data collected from sources across the hospital into a format that is ready for further analysis.  There are no user facing elements other than the data that will be provided into your project workspace within the Secure Data Environment (SDE), however our transformation pipeline is used to build the Research Data Extract and the resulting meta-data is also [documented].</p>"},{"location":"portal/Home/","title":"Data Portal","text":""},{"location":"portal/Home/#introduction","title":"Introduction","text":"<p>The Data Portal provides information on the work supported by the Barts Health Data Platform and how to access patient data through our application and review process.</p>"},{"location":"portal/Home/#application-process","title":"Application Process","text":"<p>The application process has been developed to encompass a range of different request types (i.e., research, service evaluation, clinical audit, quality improvement) that may require additional information and additional approvals from both within and outside the hospital.  If this information and approvals are not in place, then you can save a draft of the application form and return later to submit. We also support two approval types: full (when you have all the approvals and funding in place and you are ready to process data) and provisional (when you are still at the planning phase and would like an agreement in principal). You do not need to get a provisional approval before applying for full approval. Nor does a provisional approval mean that your full approval will automatically be a success, but it allows any concerns we may have with your request to be addressed ahead of the full request.</p> <p>The stages needed to complete the online form are identified below:</p> <ol> <li>Create an account on data.bartshealth.nhs.uk if you do not already have one.</li> <li>First time users will need to upload their CV and Information Governance training certificate/confirmation under their Profile.</li> <li>Once you have an account, log on to the Data Portal and select the MyDARs (Data Access Requests) tab. This is where your draft, submitted and approved requests can be found.</li> <li>In the Draft Applications section there is a button to 'Submit a Data Access Request' which will start your application process. You can move forward and backward through the form, with your overall progress through the process indicated, until you reach the final page where you will be asked to save a draft or submit the request for processing. You can return to a draft application to either view or edit the request using the menu option on each row.</li> <li>Once you have submitted your request their progress through our processes will be indicated in the middle table - Submitted DARs. These stages are:<ul> <li>DAR Submitted: The request is being validated by the internal team</li> <li>DAR Under Review: The members of the Data Access Commitee (DAC) are reviewing the request and will provide their feedback</li> <li>DAR Review Completed: All the required feedback has been provided</li> <li>Feedback to Applicants: The applicant is inivited to provide their feedback on the Reviewers feedback</li> <li>DAR Scheduled: The DAR is ready for the next DAC meeting</li> <li>DAC Approved or DAR Rejected: The DAC has met and made a decision</li> </ul> </li> <li>Once approved by the DAC they will appear in the lower table - Approved DARs - while the project administration is completed, your data prepared and your Secure Data Environment (SDE) workspace established.<ul> <li>Verifying Users: The users who will have access to the data need their identities verified and training certificates reviewed</li> <li>Release Data Release Confirmation Letter: The formal record of the data access permission</li> <li>Provisioning Project: The data needed for the project is being prepared in the dedicated SDE workspace</li> <li>Active Project: Users have been provided access to the workspace with their data</li> </ul> </li> </ol>"},{"location":"portal/form/","title":"Form","text":""},{"location":"portal/form/#data-access-request-form","title":"Data Access Request Form","text":"<p>The information the members of the Data Access Committee (DAC) need to reach an opinion on the Data Access Request (DAR) are captured through a multistage form. These stages are identified below alongside the percentage completion indicated at the top of the form: * Overvieiw (0%): Selecting the project type &amp; type, approval sought (full/prpvisional) and if the request requires any software or application to be used on the Barts Health network outside of the Secure Data Environment. * Approvals (9%): Provide details of your IRAS ID, REC/HRA and JRMO approvals. * Alemba Reference (18%): Asked if you are installing software or an application outside of the SDE. * Project Details (22%): Provide text around the project description, a lay summary of the work that will be made public on the website, a summary of the requested data, how you will analyse the data, a summary of the public and patient involvement and engagement you plan as part of the project and how you will report the work and impac tof the project. In addition you will be asked for contact details of the lead applicant, an administrative contaact and details about the organisation and if there are any partners. * Partner Information (31%): If there are any partners in this project who will employ individuals who will need access to the data, please provide their details here. * Project Scope and Staff (36%): Provide information about project staff and the scope of the collaboration. * BH and QMUL (45%): For local projects are the applicants aware of any similar projects being undertaken. * External Approvals (50%): Upload any external approvals, e.g. CAG, Consent Forms, HRA documents, etc. * Requested Data Fields (54%): The Research DataExtract specification can be found here which describes the data we currently integrate. Other data from the EPR can be found and integrated on request and this where you can detail the data you want, how we we will find it, how it may need to be linked to other data sets and any further processing that might be needed. * Secure Data Environment (59%): Will you be using the Barts Health Secure Data Envionment as part of the work. * Barts Health SDE (68%): Information on any additional software or services you might need in the SDE can be identifed here that we can provide. You also need to identify if you will need to bring additional software or data into the workspace through the airlock. * Airlock Import (77%): Provide the reason that you need to import material through the Airlock, and specify if you will need to export data through the Airlock. * Airlock Export (86%): Provide the reason you need to export data through the Airlock. * Indviduals Needing Data Access (90%): Provide details of any organisational Information Governance and Data Security training that is provided to the individuals accessing data and list the individuals accessing data.</p>"},{"location":"sde/Home/","title":"Introduction to the Secure Data Environment","text":"<p>This page contains the user documentation for the Barts Health Secure Data Environment (SDE).</p> <p>A note on terminology: We refer to a Secure Data Environment (SDE) and a Trusted Research Environment (TRE) interchangeably here. Locally, we prefer the SDE designation, but we're based on a package which calls itself a TRE. To all intents and purposes, they mean the same thing.</p>"},{"location":"sde/Home/#what-is-a-secure-data-environment","title":"What is a Secure Data Environment?","text":"<p>As the name implies, an SDE is an environment in which data can be analysed securely. This means that there is no direct access to the internet, data cannot be uploaded or downloaded freely by the User. This greatly reduces the possibility of data leaks, accidental or otherwise, and is a requirement for handling sensitive data.</p> <p>Data can only be put into or extracted from the SDE by one of two routes:</p> <ul> <li>Data from Barts Health requested and approved via the Data Access Committee (DAC) will be pushed into the project storage after any appropriate pre-processing, such as anonymisation. With the approval of the DAC, we can link data sets from other Data Controllers to our own data either anonymised or psuedoanonymised, depending on the requirements.</li> <li>Other data, software or tools can be imported or exported by going through an 'airlock' process, where the data is staged through secure storage and is reviewed by the Precision Medicine team before access is granted and the material is available the other side of the airlock.</li> </ul> <p>In order to keep the data secure, there are some restrictions on the analysis environment. The data can be accessed through virtual machines (VMs), contained in a workspace. Each workspace is unique to a project, with only project-members having access to it. The VMs in a workspace cannot be accessed directly through Secure Shell (SSH), they have to be accessed by a web-based virtual desktop. Copy/paste activity is restricted, see the airlock documentation for details. TBD</p> <p>Our SDE is based on the AzureTRE package from Microsoft. This is an implementation built by Microsoft as an 'accelerator' for the community. It's not a product in the sense that you can't click on a button in the Azure portal and have it deployed, and they don't officially support it at this time. For this reason, we have to develop it further ourselves, so that it satisfies all our requirements. See the timeline below for more details.</p>"},{"location":"sde/Home/#project-workspaces","title":"Project workspaces","text":"<p>A workspace is a container for resources, or 'services' for a project. Services include things like a Git mirror based on Gitea, a Nexus repository which allows you to access software repositories such as CRAN and PyPi, and a Guacamole Virtual Desktop service, which allows you to create virtual machines. More detail on each service is available elsewhere (TBD).</p> <p>Once a project has been approved through the DAC, we will provision a workspace for it. Users will be assigned one of two roles:</p> <ul> <li>Workspace Owner, who can create and destroy resources in the workspace, or </li> <li>Workspace Researcher, who can use resources, but can only create or destroy selected resources, such as VMs.</li> </ul> <p>A Workspace Owner can manage all the resources created by any Workspace Researcher, but a Workspace Researcher can only manage resources that they themselves create.</p> <p>You should then be able to log into the SDE, select your workspace, and view/manage/use the resources in your project.</p>"},{"location":"sde/Home/#getting-help","title":"Getting help","text":"<p>We have further information on how you can provide feedback and get support.</p>"},{"location":"sde/Home/#using-your-workspace","title":"Using your workspace","text":"<p>See Using your workspace</p>"},{"location":"sde/Using-your-workspace/","title":"Using your workspace","text":"<p>You access your workspace via the web, once you've been assigned an appropriate role. A workspace consists of several workspace services, plus also access to shared services, which are shared by all the workspaces in the SDE. The Workspace Owner can manage all the resources in their own workspaces, regardless of who created them, but cannot manipulate the shared services. A Workspace Researcher can also create and use certain resources, but can only delete or modify resources they have created themselves.</p>"},{"location":"sde/Using-your-workspace/#getting-access-to-the-sde","title":"Getting access to the SDE","text":"<p>As of December 2024, the MVP release of the SDE is now available, and will be the default mode of providing data and compute resources to projects approved through the DAC. If you have been given access to it, you can log in at https://sde.bartshealth.nhs.uk/, using your credentials. These credentials will either be:</p> <ul> <li>Your usual Barts Health credentials that you would use to log into your laptop or VDI. Generally your desktop login @ bartshealthnhs.uk (e.g.,  SmithJ@bartshealthnhs.uk)</li> <li>Dedicated credentials to access Precision Medicine related services (e.g., SmithJ@BHPrecisionMedicine.onmicrosoft.com). These will be provided if you do not have an NHS account that can be mapped into our Entra ID directory.</li> </ul> <p>TBD contact us if you think you should be among them.</p>"},{"location":"sde/Using-your-workspace/#resource-hierarchy-sde-workspace-user","title":"Resource hierarchy: SDE, Workspace, User","text":"<p>Within the SDE, resources for a project are grouped into a Workspace, which has workspace-level resources. These workspace-level resources may also have user-level resources. The terms project and workspace both refer to the same thing as far as the SDE is concerned. From within a workspace, you will not have access to any other workspaces or their resources - this is a key feature of the SDE architecture. If you have multiple workspaces available to you, by way of being on multiple projects at the same time, the only way to share data between them will be to export it from one workspace and import it to the other.</p> <p>TBD links Some examples of workspace services:</p> <ul> <li>Gitea is a git repository management system. This should be installed once in each workspace to allow all Users in that workspace to use Git to share and manage your code within that workspace. Changes you make in these git repositories will need to be exported through the Airlock to be used elsewere.</li> <li>Apache Guacamole is a virtual desktop management system. This allows you to create virtual machines (Linux or Windows), and then get a virtual desktop to access them. Guacamole itself is a project-level resource, so you only need one per workspace, but the VMs created in it are specific to the user, so they are a user-level resource, not shared among multiple users.</li> <li>MySQL is a standard SQL database service, which, again, probably only needs one deployment per workspace. Each MySQL service can hold multiple databases, each with their own permissions.</li> <li>Azure Databricks, AzureML, and MLflow are available for machine-learning projects.</li> </ul> <p>At the SDE level, the most important shared services are:</p> <ul> <li>Gitea. This shared service is useful for mirroring external Git repositories into the SDE network space so you can access them from within a workspace. Without this, you have no external access to, for example, GitHub. The access is one way, you cannot use this service to push changes back to GitHub. Access to this service can be provided by raising a support issue and identifying the GitHub repositories that need to be mirrored.</li> <li>Nexus. This is a package mirroring repository from Sonatype, which allows you to download packages from CRAN, PyPi, Ubuntu mirrors, and other sources. If you wish to access a package repository that is not currently mirrored then please raise a support issue.</li> </ul> <p>TBD Document how to use these. There are other services available, and new services can be created by building templates for them. If you have a need for a service that's not currently represented, contact us.</p>"},{"location":"sde/Using-your-workspace/#accessing-a-workspace","title":"Accessing a workspace","text":"<p>When you log into the SDE, your initial view will look something like this. There will be a separate workspace for each project you have access to, in this example, there's only one project.  TBD</p> <p>Click on the workspace to get the Workspace Overview page. There are other tabs there you can explore, to get more information about your workspace. They're not normally useful during operation, but if you file a bug report or an issue, a screenshot from the Operations tab is often helpful.  TBD</p> <p>The workspace currently has no services in it. Workspace Owners can use the Create new button on the right to bring up a menu of resources to choose from, then you fill in the form with a few parameters, and submit the form. Your workspace resources will then appear here, as you can see in the next screenshot. Here, we see a Gitea service, which is still deploying, and a Guacamole service, which is ready for use.</p> <p>TBD N.B. We don't show the full process for deploying Gitea or Guacamole, since we will do that for you as part of the setup of your workspace. In any case, the process is very similar to that for creating a virtual machine, which is explained in detail below.  TBD</p>"},{"location":"sde/Using-your-workspace/#creating-a-virtual-machine","title":"Creating a Virtual Machine","text":"<p>Virtual machines are created via the Guacamole workspace service. From your workspace overview, click on the Guacamole service tile, which takes you to the screen below. Any existing VMs that you have access to will also be listed here, there are none at the moment. </p> <p>Click the Create new button, then select an operating system, Windows or Linux.  </p> <p>The next form is practically identical for Windows or Linux machines, only the dropdown options vary. Fill in the obligatory name and description, select an image (which specifies which types of software the machine will have, what version of the operating system it runs etc), and a size, which specifies the number of CPUs and amount of memory.</p> <p>You cannot, at this time, specify the size of the disk for the virtual machine. That may be added as a feature later on, let us know if you need that capability.</p> <p>Similarly, the options for VM image and size are currently rather limited. We expect to add options in the future for VMs with GPUs etc, and we are aware of the need to customise images for different use-cases. Again, please contact us to let us know your requirements.</p> <p>Leave the Shared storage button selected. This means your VM will have access to storage that exists at the workspace-level, so will persist if you delete your VM. If you only use the VM disk, you will lose everything on it when the VM is destroyed. There's more info on that in the [[Working with data]] page.  </p> <p>Click the Submit button, and after a few seconds, you see something like the next screenshot. You can either click the Go to resource button, or just click directly on the VM tile in the Resources section, to follow the progress of the deployment.</p> <p>The deployment progresses through several stages:</p> <ul> <li>pending means the deployment is queued, but hasn't yet started</li> <li>deploying means the resource is being constructed</li> <li>running means the deployment succeeded</li> </ul> <p>Other states exist, such as failed, or updating, we don't go into those here.  </p> <p>The Operations tab shows you detail of the deployment steps. The overall progress of the deployment is visible in the top-right in this view. Other buttons are greyed out at this point, and will remain so until the deployment finishes. This typically takes 5-10 minutes, but can take longer sometimes.</p> <p> </p> <p> </p> <p>Now the VM is fully deployed, it's shown as running in the title, and the Update, Delete and Actions buttons are now active. In the Actions menu, there are three options:</p> <ul> <li>Reset password - ignore this, you don't need a password to access the VM.</li> <li>Start will start the VM if it has been stopped, and</li> <li>Stop will stop a running VM.</li> </ul> <p>You will be charged for your VM all the time it is running. If you know you won't need it for a while (e.g. overnight, weekends, or holidays), it makes sense to Stop it when you don't need it, and Start it again to save your project budget. We envisage adding the ability to auto-shutdown machines which are idle for, say, an hour, but that isn't available yet.</p> <p> </p> <p>Once the VM is fully booted, the Guacamole page will look like this. The VM is shown, with a Connect button, which will launch a new browser window connected to your VM.</p> <p> </p> <p>Clicking the Connect button takes you to your virtual desktop. In this case, it's Ubuntu. You can hover over the icons for help on each.</p> <p> </p> <p>Clicking the Terminal icon brings up a terminal window. When you're finished with your session, just close the browser tab. Reconnecting later will bring it back in the same state.</p> <p> </p>"},{"location":"sde/Using-your-workspace/#working-with-linux-vms","title":"Working with Linux VMs","text":"<ul> <li>Your Linux VM has no direct connectivity to the internet. If you need to update a package, or install new software, you can access various mirrors courtesy of the Nexus shared service, but you won't be able to download software from arbitrary sites.</li> <li>VMs are not backed up. If you delete a VM, everything on the disk is destroyed, with no possibility of recovering it.</li> <li>On the other hand, stopping and restarting the VM from the Action menu does not wipe the contents of your home directory, so is a safe way of saving money when you don't need the VM for a while. However, the /tmp directory is wiped on reboots, so don't store anything there if you want to keep it.</li> <li>The home directory is mounted on the root filesystem, which is not large. If you fill the disk completely, your VM may stop responding, and you may lose access to it, possibly losing your work in the process. For this reason, it's better to work with the shared storage we will configure into your workspace.</li> </ul>"},{"location":"sde/Using-your-workspace/#more-details-about-workspaces","title":"More details about workspaces","text":""},{"location":"sde/Using-your-workspace/#keeping-vms-up-to-date","title":"Keeping VMs up to date","text":"<p>you should make sure you update the OS and software on your VMs regularly. Even though they're contained within a firewall, it's good best practice to make sure you're not running old software. For this reason, we'll be updating the VM templates regularly, and you can take advantage of that by simply deleting your VM and creating a new one.</p> <p>This is the recommended best practice, and it requires that you think carefully about how you customise your machines. If you find that you need to put a lot of effort into customising an image, let us know, we may be able to do this for you, and automate the process.</p>"},{"location":"sde/Using-your-workspace/#considerations-for-workspace-managers","title":"Considerations for workspace managers","text":""},{"location":"sde/Using-your-workspace/#cost-efficiency","title":"Cost efficiency","text":"<p>Workspace Admins are responsible for managing the cost of their workspaces. Once the MVP goes live, users will be expected to pay for their services, until then, the cost will be absorbed by the Precision Medicine team.</p> <p>You can see the cost of your resources on the tile for that resource.  Generally, the most expensive resources are high-level managed functions like AzureML, or some of the larger VM sizes - particularly those with GPUs. Note that the cost displayed on the UI is from the Azure billing API, and may not directly reflect the cost you will be charged, the exact costing model has not yet been worked out. There is overhead for the SDE platform which has to be taken into account, not just the resources deployed into the workspace.</p> <p>User resources (e.g. VMs), workspace resources, and workspaces themselves can all be 'disabled', to save costs. Services that are disabled are hibernated, to the extent possible for that service, to minimise expenditure. You will still be charged for any storage used by a service during hibernation, but not for compute etc. Storage is generally cheap, so its can be ignored for most purposes.</p> <p>Note that disabling a service does not disable services below them. E.g., disabling Guacamole, the virtual desktop service, does not cascade down to disabling the VMs that have been deployed by it, and disabling the workspace does not disable any of the services within it. If you want to disable everything, you have to start from the bottom up, disabling user resources, then workspace services, then the workspace itself.</p> <p>We recommend that you monitor costs of your services in the first days of accessing your workspace, and decide which services to disable/re-enable based on your budget and convenience.</p> <p>We will provide the means to automatically disable VMs after a certain amount of idle-time, though that's not yet available.</p>"},{"location":"sde/Working-with-data/","title":"Working with data","text":"<p>There are two ways to get data into your workspace, and one way to get data out. Data can be provisioned as part of the workspace configuration, following approval by the DAC, or it can be imported/exported via the Airlock mechanism.</p>"},{"location":"sde/Working-with-data/#data-provisioning-via-the-dac","title":"Data provisioning via the DAC","text":"<p>TBC Something here about how the DAC pipeline pushes data into the workspace, where it lands, how the user finds it. Needs to cover both Linux and Windows clients.</p>"},{"location":"sde/Working-with-data/#the-airlock-mechanism","title":"The Airlock mechanism","text":"<p>The Airlock metaphor captures the way users can import their own data into their workspace, or export it from it. There is no direct access to allow data to be moved into/out of the workspace storage, instead, someone must review the data in each case, and approve or deny the request.</p>"},{"location":"sde/Working-with-data/#importing-data","title":"Importing data","text":"<p>For importing, the workflow looks like this:</p> <ul> <li>A researcher creates a request to import data,</li> <li>Then they upload their data to secure storage, as part of the request process,</li> <li>Then they submit the request.</li> <li>An Airlock Manager, who is appointed per project, will review the request, and approve or deny it.</li> <li>If approved, the researcher will then receive an email with a secure URL to allow them to retrieve data from the workspace, using a VM within the workspace.</li> </ul> <p>Note that there are two important restrictions on importing/exporting data:</p> <ol> <li>You can only import or export a single file at a time. If you want to transfer multiple files, you can always zip them together, that's fine.</li> <li>The file is currently limited to a maximum size of 2 GB, because of the requirements to scan it for malware with Azure Defender for Cloud. If you have data larger than 2 GB, either break it into several smaller requests, or contact us to find a solution for you.</li> </ol>"},{"location":"sde/Working-with-data/#creating-an-airlock-request","title":"Creating an airlock request","text":"<p>As a pre-requisite, you need either to have the Azure CLI installed on your laptop, or to have the Azure Storage Explorer installed. This example shows the workflow using the Storage Explorer, using the CLI is more self-explanatory, assuming you're familiar with it.</p> <p>In your workspace, in the sidebar on the left, there's an Airlock button. Click that, and you'll see the Airlock landing page. In this example, there are no requests yet. Click the New request button to create one.  </p> <p>Choose between an import request and an export request.  </p> <p>Give a name and a justification, then click Create, then confirm that you want to create the request.  </p> <p>Once your request is created, you need to upload the data before you submit the request.</p> <p>A request can only contain a single file, so if you have multiple files, create a tar or zip of them, and upload that. There are two ways to upload your single file, either using a SAS URL, or using the Azure CLI. At this point, the submission form will have refreshed and at the end of the form there are two tabs (SAS URL and CLI) that provide information needed for the next stage.</p> <p>NB: If you click away from the form, it will disappear. In that case, refresh the Airlock landing page, and you'll see your request there, in draft form. Double-click on it, and the form will open again.</p> <p> </p> <p>This example uses the SAS URL and by default the tab provides the information needed for the Azure Storage Explorer tool. The CLI tab provides a command line example you will need to complete if uploading from the command line using the Azure CLI.</p> <p>Now open the Azure Storage Explorer. If this is your first time using it, click the Sign in with Azure tile, and follow the instructions. Then come back and click Attach to a resource,   and then Blob container or directory.  </p> <p>Select Shared access signature URL (SAS), paste in the URL from your request, click Next, then Connect.  </p> <p>Use the Upload button and select your data file for upload. Leave the parameters at their defaults, and complete the upload. The Storage Explorer should then look like this, showing that your file (\"pi.txt\", in this example) has been successfully uploaded. You can also drag-and-drop into the Storage Explorer to upload your file that way.  </p> <p>Go back to the airlock form, or re-open it by double-clicking on your request in the airlock landing page, and click Submit.  </p> <p>An email will be sent to the airlock reviewer, who will then review your data to check it's safe for upload. They will approve or deny your request, and you will be notified by email of the result.  </p>"},{"location":"sde/Working-with-data/#retrieving-the-data-in-a-workspace-vm","title":"Retrieving the data in a workspace VM","text":"<p>If your request has been approved, you can double-click on the request and see the new SAS URL that you can use for downloading the data into your VM. The Azure Storage Explorer (<code>storage-explorer</code> on the Linux command line) is already installed in the workspace VM images (currently only the Data Science VMs), so you can launch it, and use the same method as above for downloading the file. The first time you run storage-explorer you will need to run:</p> <p><code>sudo snap connect storage-explorer:password-manager-service :password-manager-service</code></p> <p></p>"},{"location":"sde/Working-with-data/#exporting-data","title":"Exporting data","text":"<p>The export process is entirely analogous, simply starting from the workspace VM with the upload of a (possibly tarred or zipped) file.</p>"},{"location":"sde/Working-with-data/#shared-storage","title":"Shared Storage","text":"<p>There's also the possibility of using shared storage to share files between multiple VMs. E.g., if you have a Windows VM and a Linux VM, so you can use OS-specific tools, you may want to share your data between those machines.</p> <p>A workspace can be configured with shared storage, or without. By default, we will configure a reasonable amount of shared storage for you, though if you know of specific needs, let us know. Shared storage is valuable because it will outlive the individual VMs, if you create a file on shared storage on a VM, delete the VM, then create a new VM, it will find the file still there.</p> <p>On Windows VMs, the shared storage is be mounted on the <code>Z:</code> drive. On Linux VMs, the shared storage is mounted under the <code>/shared-storage</code> folder.</p> <p>The files in this folder are always owned by the currently logged-in user, whichever OS you're using, even if you're using multiple machines, so there's no concept (yet) of a proper multi-user filesystem.</p>"},{"location":"subdir/test/","title":"This is a test","text":"<p>insert useful documentation here, please...</p>"}]}